{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T21:40:37.975205Z",
     "start_time": "2024-12-03T21:40:29.706781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper function to convert RGB to segment ID\n",
    "def rgb_to_segment_id(rgb_image):\n",
    "    rgb = np.array(rgb_image)\n",
    "    segment_id = rgb[:, :, 0].astype(np.uint32) * 256 * 256 + \\\n",
    "                rgb[:, :, 1].astype(np.uint32) * 256 + \\\n",
    "                rgb[:, :, 2].astype(np.uint32)\n",
    "    return segment_id\n",
    "\n",
    "class COCOPanopticDataset(Dataset):\n",
    "    def __init__(self, images_dir, panoptic_dir, annotations_file, transform=None, target_transform=None, num_classes=201):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with all the images (.jpg).\n",
    "            panoptic_dir (str): Directory with all the panoptic segmentation masks (.png).\n",
    "            annotations_file (str): Path to the COCO panoptic annotations JSON file.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            target_transform (callable, optional): Optional transform to be applied on the mask.\n",
    "            num_classes (int): Number of segmentation classes.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.panoptic_dir = panoptic_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Load the panoptic annotations JSON\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Create a mapping from image_id to annotation\n",
    "        self.image_id_to_ann = {ann['image_id']: ann for ann in self.annotations['annotations']}\n",
    "\n",
    "        # Extract all image entries\n",
    "        self.images = self.annotations['images']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image information\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info['id']\n",
    "        img_filename = img_info['file_name']\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get corresponding panoptic annotation\n",
    "        ann = self.image_id_to_ann.get(img_id, None)\n",
    "        if ann is None:\n",
    "            raise ValueError(f\"No annotation found for image_id {img_id}\")\n",
    "\n",
    "        panoptic_filename = ann['file_name']\n",
    "        panoptic_path = os.path.join(self.panoptic_dir, panoptic_filename)\n",
    "\n",
    "        # Load panoptic segmentation mask\n",
    "        panoptic_image = Image.open(panoptic_path).convert('RGB')\n",
    "        segment_id = rgb_to_segment_id(panoptic_image)\n",
    "\n",
    "        # Initialize mask\n",
    "        height, width = image.size[1], image.size[0]\n",
    "        mask = np.zeros((height, width), dtype=np.int64)\n",
    "\n",
    "        # Assign class indices based on segments_info\n",
    "        for segment in ann['segments_info']:\n",
    "            cat_id = segment['category_id']\n",
    "            seg_id = segment['id']\n",
    "            # Convert category_id to zero-based index if necessary\n",
    "            class_index = cat_id - 1  # Assuming category_id starts at 1\n",
    "            mask[segment_id == seg_id] = class_index\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ],
   "id": "cd75e258d2c9fe1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T21:40:38.007076Z",
     "start_time": "2024-12-03T21:40:37.989078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Desired dimensions\n",
    "H, W = 256, 256  # Adjust based on your model's requirements\n",
    "\n",
    "# Image transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((H, W)),  # Resize images to H x W\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "\n",
    "# Mask transformations\n",
    "def mask_transform(mask):\n",
    "    \"\"\"\n",
    "    Transforms the mask by resizing and converting it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        mask (numpy.ndarray): The segmentation mask.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The transformed mask.\n",
    "    \"\"\"\n",
    "    mask = Image.fromarray(mask)\n",
    "    mask = transforms.Resize((H, W), interpolation=Image.NEAREST)(mask)  # Resize with nearest neighbor to preserve labels\n",
    "    mask = np.array(mask, dtype=np.int64)\n",
    "    return torch.from_numpy(mask)\n"
   ],
   "id": "4709a1f4a7865af3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T21:40:42.312756Z",
     "start_time": "2024-12-03T21:40:38.019077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths (replace with your actual paths)\n",
    "images_dir = 'C:/Users/gades/MLProject/Datasets/COCOPanoptic/COCOPanoptic/images/train2017'  # Directory containing .jpg images\n",
    "panoptic_dir = 'C:/Users/gades/MLProject/Datasets/COCOPanoptic/COCOPanoptic/panoptic_train2017'  # Directory containing .png masks\n",
    "annotations_file = 'C:/Users/gades/MLProject/Datasets/COCOPanoptic/COCOPanoptic/annotations/panoptic_train2017.json'  # Path to panoptic_train2017.json\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = COCOPanopticDataset(\n",
    "    images_dir=images_dir,\n",
    "    panoptic_dir=panoptic_dir,\n",
    "    annotations_file=annotations_file,\n",
    "    transform=image_transforms,\n",
    "    target_transform=mask_transform,\n",
    "    num_classes=201\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the DataLoader\n",
    "batch_size = 8  # Adjust based on your GPU memory\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ],
   "id": "aaa9bbe7692e21cd",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
