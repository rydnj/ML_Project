{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1740174d2bbced5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T04:39:50.459596Z",
     "start_time": "2024-12-04T04:39:46.945901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in dataloader: 14786\n",
      "segment_id: [[  5   5   5 ...   5   5   5]\n",
      " [  5   5   5 ...   5   5   5]\n",
      " [  5   5   5 ...   5   5   5]\n",
      " ...\n",
      " [120 120 120 ... 120 120 120]\n",
      " [120 120 120 ... 120 120 120]\n",
      " [120 120 120 ... 120 120 120]]\n",
      "Unique segment IDs: [  5 120 127]\n",
      "Number of unique segment IDs: 3\n",
      "One-hot encoded mask unique values (first image): tensor([0.])\n",
      "segment_id: [[  0   0   0 ...  26  26  26]\n",
      " [  0   0   0 ...  26  26  26]\n",
      " [  0   0   0 ...  26  26  26]\n",
      " ...\n",
      " [  0   0   0 ... 185 185 185]\n",
      " [  0   0   0 ... 185 185 185]\n",
      " [  0   0   0 ... 185 185 185]]\n",
      "Unique segment IDs: [  0  26  37  38  44  51  62  85 109 139 185 192 194]\n",
      "Number of unique segment IDs: 13\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[ 27  27  27 ...   0   0   0]\n",
      " [ 27  27  27 ...   0   0   0]\n",
      " [ 27  27  27 ...   0   0   0]\n",
      " ...\n",
      " [115 115 115 ...   0   0   0]\n",
      " [115 115 115 ...   0   0   0]\n",
      " [115 115 115 ...   0   0   0]]\n",
      "Unique segment IDs: [  0  27  41  46 107 115 140 164]\n",
      "Number of unique segment IDs: 8\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[145 145 145 ... 145 145 145]\n",
      " [145 145 145 ... 145 145 145]\n",
      " [145 145 145 ... 145 145 145]\n",
      " ...\n",
      " [ 75  75  75 ...  75  75  75]\n",
      " [ 75  75  75 ...  75  75  75]\n",
      " [ 75  75  75 ...  75  75  75]]\n",
      "Unique segment IDs: [  0   4  13  33  44  45  75  80 101 110 113 115 117 119 135 141 145 156\n",
      " 161 164 169 178 191]\n",
      "Number of unique segment IDs: 23\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[40 40 40 ... 40 40 40]\n",
      " [40 40 40 ... 40 40 40]\n",
      " [40 40 40 ... 40 40 40]\n",
      " ...\n",
      " [87 87 87 ... 87 87 87]\n",
      " [87 87 87 ... 87 87 87]\n",
      " [87 87 87 ... 87 87 87]]\n",
      "Unique segment IDs: [  0  24  25  28  40  74  82  87  96 116 125 126 174 176 187 195]\n",
      "Number of unique segment IDs: 16\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[ 89  89  89 ... 170 170 170]\n",
      " [ 89  89  89 ... 170 170 170]\n",
      " [ 89  89  89 ... 170 170 170]\n",
      " ...\n",
      " [ 89  89  89 ...   0   0   0]\n",
      " [ 89  89  89 ...   0   0   0]\n",
      " [ 89  89  89 ...   0   0   0]]\n",
      "Unique segment IDs: [  0   8  10  28  39  40  64  65  68  72  89  93  98 100 101 104 135 142\n",
      " 156 158 167 170 194 195 198 199 200]\n",
      "Number of unique segment IDs: 27\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[66 66 66 ... 66 66 66]\n",
      " [66 66 66 ... 66 66 66]\n",
      " [66 66 66 ... 66 66 66]\n",
      " ...\n",
      " [66 66 66 ... 66 66 66]\n",
      " [66 66 66 ... 66 66 66]\n",
      " [66 66 66 ... 66 66 66]]\n",
      "Unique segment IDs: [ 0 30 37 66]\n",
      "Number of unique segment IDs: 4\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "segment_id: [[188 188 188 ... 188 188 188]\n",
      " [188 188 188 ... 188 188 188]\n",
      " [188 188 188 ... 188 188 188]\n",
      " ...\n",
      " [ 56  56  56 ...  56  56  56]\n",
      " [ 56  56  56 ...  56  56  56]\n",
      " [ 56  56  56 ...  56  56  56]]\n",
      "Unique segment IDs: [  0  48  56  83 134 157 159 188 200]\n",
      "Number of unique segment IDs: 9\n",
      "One-hot encoded mask unique values (first image): tensor([0., 1.])\n",
      "Batch 1:\n",
      "Image batch shape: torch.Size([8, 3, 256, 256])\n",
      "One-hot encoded mask shape: torch.Size([8, 201, 256, 256])\n",
      "Is images a tensor? True\n",
      "Is one-hot encoded mask a tensor? True\n",
      "Sample one-hot encoded mask: tensor([[[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def rgb_to_segment_id(rgb_image, num_classes=201):\n",
    "    rgb = np.array(rgb_image)\n",
    "    segment_id = (\n",
    "        rgb[:, :, 0].astype(np.uint32) * 256 * 256\n",
    "        + rgb[:, :, 1].astype(np.uint32) * 256\n",
    "        + rgb[:, :, 2].astype(np.uint32)\n",
    "    )\n",
    "    segment_id = segment_id % num_classes\n",
    "    return segment_id\n",
    "\n",
    "\n",
    "def one_hot_encode_from_segment_id(segment_id, num_classes=201, target_classes=201):\n",
    "    segment_id = torch.tensor(segment_id.astype(np.int64), dtype=torch.long)  # Convert to tensor\n",
    "    unique_segment_ids = np.unique(segment_id)\n",
    "    print(f\"Unique segment IDs: {unique_segment_ids}\")\n",
    "    print(f\"Number of unique segment IDs: {len(unique_segment_ids)}\")\n",
    "    try:\n",
    "        y_one_hot = torch.nn.functional.one_hot(segment_id, num_classes=num_classes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during one-hot encoding: {e}\")\n",
    "        raise  # [H, W, num_classes]\n",
    "    y_one_hot = y_one_hot.permute(2, 0, 1).float()  # [1, num_classes, H, W]\n",
    "\n",
    "    print(f\"One-hot encoded mask unique values (first image): {torch.unique(y_one_hot[0])}\")\n",
    "\n",
    "    if target_classes > num_classes:\n",
    "        padding = torch.zeros(\n",
    "            (y_one_hot.size(0), target_classes - num_classes, y_one_hot.size(2), y_one_hot.size(3)),\n",
    "            device=y_one_hot.device,\n",
    "        )\n",
    "        y_one_hot = torch.cat([y_one_hot, padding], dim=1)\n",
    "    elif target_classes < num_classes:\n",
    "        y_one_hot = y_one_hot[:, :target_classes, :, :]\n",
    "\n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "class COCOPanopticDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        panoptic_dir,\n",
    "        annotations_file,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        num_classes=201,\n",
    "        resize=(256, 256),\n",
    "    ):\n",
    "        self.images_dir = images_dir\n",
    "        self.panoptic_dir = panoptic_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_classes = num_classes\n",
    "        self.resize = resize\n",
    "        with open(annotations_file, \"r\") as f:\n",
    "            self.annotations = json.load(f)\n",
    "        self.image_id_to_ann = {ann[\"image_id\"]: ann for ann in self.annotations[\"annotations\"]}\n",
    "        self.images = self.annotations[\"images\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_id = img_info[\"id\"]\n",
    "        img_filename = img_info[\"file_name\"]\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "        ann = self.image_id_to_ann.get(img_id, None)\n",
    "        if ann is None:\n",
    "            raise ValueError(f\"No annotation found for image_id {img_id}\")\n",
    "\n",
    "        panoptic_filename = ann[\"file_name\"]\n",
    "        panoptic_path = os.path.join(self.panoptic_dir, panoptic_filename)\n",
    "\n",
    "        if not os.path.exists(panoptic_path):\n",
    "            raise FileNotFoundError(f\"Panoptic mask not found: {panoptic_filename}\")\n",
    "\n",
    "        try:\n",
    "            panoptic_image = Image.open(panoptic_path)\n",
    "        except Exception as e:\n",
    "            raise\n",
    "        # Resize panoptic image to match the image size\n",
    "        panoptic_image = panoptic_image.resize(self.resize, Image.NEAREST)\n",
    "        segment_id = rgb_to_segment_id(panoptic_image)\n",
    "        print(f\"segment_id: {segment_id}\")\n",
    "\n",
    "        # Directly convert segment_id to one-hot encoding\n",
    "        y_one_hot = one_hot_encode_from_segment_id(\n",
    "            segment_id, num_classes=self.num_classes, target_classes=self.num_classes\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            y_one_hot = self.target_transform(y_one_hot)\n",
    "\n",
    "        return image, y_one_hot\n",
    "\n",
    "\n",
    "H, W = 256, 256\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((H, W)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "images_dir = \"data/COCOPanoptic/COCOPanoptic/images/train2017\"\n",
    "panoptic_dir = \"data/COCOPanoptic/COCOPanoptic/annotations/panoptic_train2017\"\n",
    "annotations_file = \"data/COCOPanoptic/COCOPanoptic/annotations/panoptic_train2017.json\"\n",
    "\n",
    "dataset = COCOPanopticDataset(\n",
    "    images_dir=images_dir,\n",
    "    panoptic_dir=panoptic_dir,\n",
    "    annotations_file=annotations_file,\n",
    "    transform=image_transforms,\n",
    "    num_classes=201,\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(f\"Number of batches in dataloader: {len(dataloader)}\")\n",
    "for idx, (images, y_one_hot) in enumerate(dataloader):\n",
    "    print(f\"Batch {idx + 1}:\")\n",
    "    print(f\"Image batch shape: {images.shape}\")  # Expected: [batch_size, 3, H, W]\n",
    "    print(f\"One-hot encoded mask shape: {y_one_hot.shape}\")  # Expected: [batch_size, target_classes, H, W]\n",
    "    print(f\"Is images a tensor? {torch.is_tensor(images)}\")\n",
    "    print(f\"Is one-hot encoded mask a tensor? {torch.is_tensor(y_one_hot)}\")\n",
    "    print(f\"Sample one-hot encoded mask: {y_one_hot[1]}\")  # Print first one-hot encoded mask for inspection\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89632a87c72cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
